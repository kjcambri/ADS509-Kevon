{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a887de5d",
   "metadata": {},
   "source": [
    "# Assignment 1: Data Acquisition with APIs and Scraping\n",
    "## Kevon Cambridge\n",
    "### Artist picked:\n",
    "1. Dermot Kennedy\n",
    "2. Damian Marley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b805586",
   "metadata": {},
   "source": [
    "##### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39ba20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter Section\n",
    "import tweepy\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "#Lyrics Scrape Section\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0442f",
   "metadata": {},
   "source": [
    "##### API Key Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e522a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kevon_twitter_api_keys import api_key, api_key_secret, bearer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5e518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5411fc",
   "metadata": {},
   "source": [
    "##### Testing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c7174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = \"37chandler\"\n",
    "user_obj = client.get_user(username=handle)\n",
    "\n",
    "followers = client.get_users_followers(\n",
    "    # Learn about user fields here: \n",
    "    # https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/user\n",
    "    user_obj.data.id, user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                                   \"public_metrics\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d34778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'37chandler'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc413d",
   "metadata": {},
   "source": [
    "##### Printing out names, locations, following counts and followers count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19037f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave Renn lists 'None' as their location.\n",
      " Following: 42, Followers: 10.\n",
      "\n",
      "Lionel lists 'None' as their location.\n",
      " Following: 202, Followers: 204.\n",
      "\n",
      "Megan Randall lists 'None' as their location.\n",
      " Following: 141, Followers: 100.\n",
      "\n",
      "Jacob Salzman lists 'None' as their location.\n",
      " Following: 562, Followers: 134.\n",
      "\n",
      "twiter not fun lists 'None' as their location.\n",
      " Following: 221, Followers: 21.\n",
      "\n",
      "Hariettwilsonincarnate lists 'None' as their location.\n",
      " Following: 218, Followers: 60.\n",
      "\n",
      "Christian Tinsley lists 'None' as their location.\n",
      " Following: 2, Followers: 0.\n",
      "\n",
      "Steve lists 'I'm over here.' as their location.\n",
      " Following: 1591, Followers: 33.\n",
      "\n",
      "John O'Connor ðŸ‡ºðŸ‡¦ lists 'None' as their location.\n",
      " Following: 8, Followers: 1.\n",
      "\n",
      "CodeGrade lists 'Amsterdam' as their location.\n",
      " Following: 2820, Followers: 424.\n",
      "\n",
      "Cleverhood lists 'Providence, RI' as their location.\n",
      " Following: 2795, Followers: 3563.\n",
      "\n",
      "Regina ðŸš¶â€â™€ï¸ðŸš²ðŸŒ³ lists 'Minneapolis' as their location.\n",
      " Following: 2802, Followers: 3336.\n",
      "\n",
      "Eric Hallstrom lists 'Missoula, MT' as their location.\n",
      " Following: 464, Followers: 305.\n",
      "\n",
      "Tyler ðŸ“Š ðŸ• ðŸš² lists 'Minneapolis, MN' as their location.\n",
      " Following: 528, Followers: 83.\n",
      "\n",
      "The Center for Community Ownership (CCO) lists 'None' as their location.\n",
      " Following: 53, Followers: 41.\n",
      "\n",
      "Deepak Chauhan lists 'None' as their location.\n",
      " Following: 450, Followers: 25.\n",
      "\n",
      "Patsy lists 'Seattle, WA' as their location.\n",
      " Following: 156, Followers: 15.\n",
      "\n",
      "andrew lists 'St Paul, MN' as their location.\n",
      " Following: 1413, Followers: 461.\n",
      "\n",
      "Ada Smith lists 'None' as their location.\n",
      " Following: 274, Followers: 198.\n",
      "\n",
      "Stacey Burns lists 'Minneapolis Witch District' as their location.\n",
      " Following: 4586, Followers: 10884.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_to_print = 20\n",
    "\n",
    "for idx, user in enumerate(followers.data) :\n",
    "    following_count = user.public_metrics['following_count']\n",
    "    followers_count = user.public_metrics['followers_count']\n",
    "    \n",
    "    print(f\"{user.name} lists '{user.location}' as their location.\")\n",
    "    print(f\" Following: {following_count}, Followers: {followers_count}.\")\n",
    "    print()\n",
    "    \n",
    "    if idx >= (num_to_print - 1) :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa8d746",
   "metadata": {},
   "source": [
    "##### Finding person with the most followers who follows this handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f9d161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WedgeLIVE\n",
      "{'followers_count': 14180, 'following_count': 2221, 'tweet_count': 56103, 'listed_count': 218}\n"
     ]
    }
   ],
   "source": [
    "max_followers = 0\n",
    "\n",
    "for idx, user in enumerate(followers.data) :\n",
    "    followers_count = user.public_metrics['followers_count']\n",
    "    \n",
    "    if followers_count > max_followers :\n",
    "        max_followers = followers_count\n",
    "        max_follower_user = user\n",
    "\n",
    "        \n",
    "print(max_follower_user)\n",
    "print(max_follower_user.public_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d327af1",
   "metadata": {},
   "source": [
    "##### printing out user_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d346d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.get_user(id=user_obj.data.id,\n",
    "                          user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                                       \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
    "                                       \"verified\",\"public_metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a707352a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for location we have MN\n",
      "for username we have 37chandler\n",
      "for name we have John Chandler\n",
      "for id we have 33029025\n",
      "for public_metrics we have {'followers_count': 193, 'following_count': 589, 'tweet_count': 997, 'listed_count': 3}\n",
      "for description we have He/Him. Data scientist, urban cyclist, educator, erstwhile frisbee player. \n",
      "\n",
      "Â¯\\_(ãƒ„)_/Â¯\n",
      "for created_at we have 2009-04-18 22:08:22+00:00\n",
      "for profile_image_url we have https://pbs.twimg.com/profile_images/2680483898/b30ae76f909352dbae5e371fb1c27454_normal.png\n",
      "for verified we have False\n"
     ]
    }
   ],
   "source": [
    "for field, value in response.data.items() :\n",
    "    print(f\"for {field} we have {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089f030",
   "metadata": {},
   "source": [
    "###### Q: How many fields are being returned in the response Object?\n",
    "A: We have 8 fields being returned in the response object\n",
    "\n",
    "###### Q: Are any of the fields within the user object non-scalar? \n",
    "A: Yes, the created_at is a non-scalar object. It is a datetime datatype.\n",
    "\n",
    "###### Q: How many friends, followers, and tweets does this user have?\n",
    "A: This person have 193 followers and friends and 994 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18272025",
   "metadata": {},
   "source": [
    "##### Looking at a few tweets from this user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db024c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569760631548690437\n",
      "RT @dtmooreeditor: So there's a particular quirk of English grammar that I've always found quite endearing: the exocentric verb-noun compouâ€¦\n",
      "\n",
      "1569155273742327811\n",
      "As a Minneapolis person, I knew we had Toronto beat, but I didn't realize Portland had us beat: https://t.co/xrx5mOFcWK.\n",
      "\n",
      "But @nytimes, c'mon! https://t.co/M9mBWhdgsj\n",
      "\n",
      "1568982292923826176\n",
      "RT @wonderofscience: Amazing lenticular cloud over Mount Fuji\n",
      "\n",
      "Credit: Iurie Belegurschi\n",
      "https://t.co/0mUxl28H9U\n",
      "\n",
      "1568242374085869570\n",
      "RT @depthsofwiki: lots of memes about speedy wikipedia editors â€” quick thread about what went down on wikipedia in the minutes after her deâ€¦\n",
      "\n",
      "1568074978754703361\n",
      "@DrLaurenWilson @leighradwood @MaritsaGeorgiou @Walgreens I could not possibly agree more with this sentiment. Compared to almost any other primary care I've received, they are great.\n",
      "\n",
      "1567530169686196224\n",
      "@DrLaurenWilson @MaritsaGeorgiou @Walgreens For those who have access to Curry Health Center on campus, you can get a bivalent booster in 15 minutes from their delightful staff.\n",
      "\n",
      "1567511181526708224\n",
      "RT @shes_the_maNN1: I canâ€™t describe how ancient this makes me feel. https://t.co/a1IvELjOFY\n",
      "\n",
      "1567510612665864193\n",
      "RT @AngryBlackLady: this is hilarious\n",
      "\n",
      "1566031636457725953\n",
      "RT @MarkJacob16: With all the arguments over whether MAGA Republicans are fascists, I reread William Shirerâ€™s â€œThe Rise and Fall of the Thiâ€¦\n",
      "\n",
      "1563737816219000832\n",
      "RT @wonderofscience: The Milky Way galaxy and a phenomenon known as \"airglow\" seen from the International Space Station. https://t.co/bOLt8â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.get_users_tweets(user_obj.data.id)\n",
    "\n",
    "# By default, only the ID and text fields of each Tweet will be returned\n",
    "for idx, tweet in enumerate(response.data) :\n",
    "    print(tweet.id)\n",
    "    print(tweet.text)\n",
    "    print()\n",
    "    \n",
    "    if idx > 10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e77a4",
   "metadata": {},
   "source": [
    "### Pulling Follower Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218935ac",
   "metadata": {},
   "source": [
    "##### Rate Limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23649ce",
   "metadata": {},
   "source": [
    "###### This block of code was really helpful for my situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a3d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_followers = []\n",
    "pulls = 0\n",
    "max_pulls = 100\n",
    "next_token = None\n",
    "\n",
    "while True :\n",
    "\n",
    "    followers = client.get_users_followers(\n",
    "        user_obj.data.id, \n",
    "        max_results=1000, # when you do this for real, set this to 1000!\n",
    "        pagination_token = next_token,\n",
    "        user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                     \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
    "                     \"verified\",\"public_metrics\"]\n",
    "    )\n",
    "    pulls += 1\n",
    "    \n",
    "    for follower in followers.data : \n",
    "        follower_row = (follower.id,follower.name,follower.created_at,follower.description)\n",
    "        handle_followers.append(follower_row)\n",
    "    \n",
    "    if 'next_token' in followers.meta and pulls < max_pulls :\n",
    "        next_token = followers.meta['next_token']\n",
    "    else : \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ba75c",
   "metadata": {},
   "source": [
    "#### Pulling Twitter Data for Dermot Kennedy & Damian Marley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f7ebe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would take 1.80 hours to pull all 107940 followers for DermotKennedy. \n",
      "It would take 9.07 hours to pull all 544394 followers for damianmarley. \n"
     ]
    }
   ],
   "source": [
    "artists = dict()\n",
    "handles = ['DermotKennedy','damianmarley']\n",
    "for handle in handles: \n",
    "    user_obj = client.get_user(username=handle,user_fields=[\"public_metrics\"])\n",
    "    artists[handle] = (user_obj.data.id, \n",
    "                       handle,\n",
    "                       user_obj.data.public_metrics['followers_count'])\n",
    "    \n",
    "\n",
    "for artist, data in artists.items() : \n",
    "    print(f\"It would take {data[2]/(1000*15*4):.2f} hours to pull all {data[2]} followers for {artist}. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79668ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DermotKennedy': (147529097, 'DermotKennedy', 107940),\n",
       " 'damianmarley': (245871948, 'damianmarley', 544394)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234af20",
   "metadata": {},
   "source": [
    "#### Making folder to write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6349d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "dirname = \"twitter\"\n",
    "\n",
    "if not os.path.isdir(dirname) : \n",
    "    #shutil.rmtree(\"twitter/\")\n",
    "    os.mkdir(\"twitter\")\n",
    "\n",
    "#Checking to see if the folder exist\n",
    "isdir = os.path.isdir(dirname)\n",
    "print(isdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9843f",
   "metadata": {},
   "source": [
    "#### Writing artist data to twitter folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55a9821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limiting number of followers to pull. We are limiting to 200,000 followers\n",
    "num_followers_to_pull = 200*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915bb4ff",
   "metadata": {},
   "source": [
    "##### Pulling data and inserting in text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e63b116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling followers for damianmarley.\n",
      "pulled 141 for damianmarley...\n",
      "pulled 142 for damianmarley...\n",
      "pulled 143 for damianmarley...\n",
      "pulled 144 for damianmarley...\n",
      "pulled 145 for damianmarley...\n",
      "pulled 146 for damianmarley...\n",
      "pulled 147 for damianmarley...\n",
      "pulled 148 for damianmarley...\n",
      "pulled 149 for damianmarley...\n",
      "pulled 150 for damianmarley...\n",
      "pulled 151 for damianmarley...\n",
      "pulled 152 for damianmarley...\n",
      "pulled 153 for damianmarley...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 13 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulled 154 for damianmarley...\n",
      "pulled 155 for damianmarley...\n",
      "pulled 156 for damianmarley...\n",
      "pulled 157 for damianmarley...\n",
      "pulled 158 for damianmarley...\n",
      "pulled 159 for damianmarley...\n",
      "pulled 160 for damianmarley...\n",
      "pulled 161 for damianmarley...\n",
      "pulled 162 for damianmarley...\n",
      "pulled 163 for damianmarley...\n",
      "pulled 164 for damianmarley...\n",
      "pulled 165 for damianmarley...\n",
      "pulled 166 for damianmarley...\n",
      "pulled 167 for damianmarley...\n",
      "pulled 168 for damianmarley...\n",
      "pulled 169 for damianmarley...\n",
      "pulled 170 for damianmarley...\n",
      "pulled 171 for damianmarley...\n",
      "pulled 172 for damianmarley...\n",
      "pulled 173 for damianmarley...\n",
      "pulled 174 for damianmarley...\n",
      "pulled 175 for damianmarley...\n",
      "pulled 176 for damianmarley...\n",
      "pulled 177 for damianmarley...\n",
      "pulled 178 for damianmarley...\n",
      "pulled 179 for damianmarley...\n",
      "pulled 180 for damianmarley...\n",
      "pulled 181 for damianmarley...\n",
      "pulled 182 for damianmarley...\n",
      "pulled 183 for damianmarley...\n",
      "pulled 184 for damianmarley...\n",
      "pulled 185 for damianmarley...\n",
      "pulled 186 for damianmarley...\n",
      "pulled 187 for damianmarley...\n",
      "pulled 188 for damianmarley...\n",
      "pulled 189 for damianmarley...\n",
      "pulled 190 for damianmarley...\n",
      "pulled 191 for damianmarley...\n",
      "pulled 192 for damianmarley...\n",
      "pulled 193 for damianmarley...\n",
      "pulled 194 for damianmarley...\n",
      "pulled 195 for damianmarley...\n",
      "pulled 196 for damianmarley...\n",
      "pulled 197 for damianmarley...\n",
      "pulled 198 for damianmarley...\n",
      "pulled 199 for damianmarley...\n",
      "pulled 200 for damianmarley...\n",
      "pulled 201 for damianmarley...\n",
      "pulled 202 for damianmarley...\n",
      "pulled 203 for damianmarley...\n",
      "pulled 204 for damianmarley...\n",
      "pulled 205 for damianmarley...\n",
      "pulled 206 for damianmarley...\n",
      "pulled 207 for damianmarley...\n",
      "pulled 208 for damianmarley...\n",
      "pulled 209 for damianmarley...\n",
      "pulled 210 for damianmarley...\n",
      "pulled 211 for damianmarley...\n",
      "pulled 212 for damianmarley...\n",
      "pulled 213 for damianmarley...\n",
      "pulled 214 for damianmarley...\n",
      "pulled 215 for damianmarley...\n",
      "pulled 216 for damianmarley...\n",
      "pulled 217 for damianmarley...\n",
      "pulled 218 for damianmarley...\n",
      "pulled 219 for damianmarley...\n",
      "pulled 220 for damianmarley...\n",
      "pulled 221 for damianmarley...\n",
      "pulled 222 for damianmarley...\n",
      "pulled 223 for damianmarley...\n",
      "pulled 224 for damianmarley...\n",
      "pulled 225 for damianmarley...\n",
      "pulled 226 for damianmarley...\n",
      "pulled 227 for damianmarley...\n",
      "pulled 228 for damianmarley...\n",
      "pulled 229 for damianmarley...\n",
      "pulled 230 for damianmarley...\n",
      "pulled 231 for damianmarley...\n",
      "pulled 232 for damianmarley...\n",
      "pulled 233 for damianmarley...\n",
      "pulled 234 for damianmarley...\n",
      "pulled 235 for damianmarley...\n",
      "pulled 236 for damianmarley...\n",
      "pulled 237 for damianmarley...\n",
      "pulled 238 for damianmarley...\n",
      "pulled 239 for damianmarley...\n",
      "pulled 240 for damianmarley...\n",
      "finished pulling followers for damianmarley.\n",
      "pulling followers for DermotKennedy.\n",
      "pulled 241 for DermotKennedy...\n",
      "pulled 242 for DermotKennedy...\n",
      "pulled 243 for DermotKennedy...\n",
      "pulled 244 for DermotKennedy...\n",
      "pulled 245 for DermotKennedy...\n",
      "pulled 246 for DermotKennedy...\n",
      "pulled 247 for DermotKennedy...\n",
      "pulled 248 for DermotKennedy...\n",
      "pulled 249 for DermotKennedy...\n",
      "pulled 250 for DermotKennedy...\n",
      "pulled 251 for DermotKennedy...\n",
      "pulled 252 for DermotKennedy...\n",
      "pulled 253 for DermotKennedy...\n",
      "pulled 254 for DermotKennedy...\n",
      "pulled 255 for DermotKennedy...\n",
      "pulled 256 for DermotKennedy...\n",
      "pulled 257 for DermotKennedy...\n",
      "pulled 258 for DermotKennedy...\n",
      "pulled 259 for DermotKennedy...\n",
      "pulled 260 for DermotKennedy...\n",
      "pulled 261 for DermotKennedy...\n",
      "pulled 262 for DermotKennedy...\n",
      "pulled 263 for DermotKennedy...\n",
      "pulled 264 for DermotKennedy...\n",
      "pulled 265 for DermotKennedy...\n",
      "pulled 266 for DermotKennedy...\n",
      "pulled 267 for DermotKennedy...\n",
      "pulled 268 for DermotKennedy...\n",
      "pulled 269 for DermotKennedy...\n",
      "pulled 270 for DermotKennedy...\n",
      "pulled 271 for DermotKennedy...\n",
      "pulled 272 for DermotKennedy...\n",
      "pulled 273 for DermotKennedy...\n",
      "pulled 274 for DermotKennedy...\n",
      "pulled 275 for DermotKennedy...\n",
      "pulled 276 for DermotKennedy...\n",
      "pulled 277 for DermotKennedy...\n",
      "pulled 278 for DermotKennedy...\n",
      "pulled 279 for DermotKennedy...\n",
      "pulled 280 for DermotKennedy...\n",
      "pulled 281 for DermotKennedy...\n",
      "pulled 282 for DermotKennedy...\n",
      "pulled 283 for DermotKennedy...\n",
      "pulled 284 for DermotKennedy...\n",
      "pulled 285 for DermotKennedy...\n",
      "pulled 286 for DermotKennedy...\n",
      "pulled 287 for DermotKennedy...\n",
      "pulled 288 for DermotKennedy...\n",
      "pulled 289 for DermotKennedy...\n",
      "pulled 290 for DermotKennedy...\n",
      "pulled 291 for DermotKennedy...\n",
      "pulled 292 for DermotKennedy...\n",
      "pulled 293 for DermotKennedy...\n",
      "pulled 294 for DermotKennedy...\n",
      "pulled 295 for DermotKennedy...\n",
      "pulled 296 for DermotKennedy...\n",
      "pulled 297 for DermotKennedy...\n",
      "pulled 298 for DermotKennedy...\n",
      "pulled 299 for DermotKennedy...\n",
      "pulled 300 for DermotKennedy...\n",
      "pulled 301 for DermotKennedy...\n",
      "pulled 302 for DermotKennedy...\n",
      "pulled 303 for DermotKennedy...\n",
      "pulled 304 for DermotKennedy...\n",
      "pulled 305 for DermotKennedy...\n",
      "pulled 306 for DermotKennedy...\n",
      "pulled 307 for DermotKennedy...\n",
      "pulled 308 for DermotKennedy...\n",
      "pulled 309 for DermotKennedy...\n",
      "pulled 310 for DermotKennedy...\n",
      "pulled 311 for DermotKennedy...\n",
      "pulled 312 for DermotKennedy...\n",
      "pulled 313 for DermotKennedy...\n",
      "pulled 314 for DermotKennedy...\n",
      "pulled 315 for DermotKennedy...\n",
      "pulled 316 for DermotKennedy...\n",
      "pulled 317 for DermotKennedy...\n",
      "pulled 318 for DermotKennedy...\n",
      "pulled 319 for DermotKennedy...\n",
      "pulled 320 for DermotKennedy...\n",
      "pulled 321 for DermotKennedy...\n",
      "pulled 322 for DermotKennedy...\n",
      "pulled 323 for DermotKennedy...\n",
      "pulled 324 for DermotKennedy...\n",
      "pulled 325 for DermotKennedy...\n",
      "pulled 326 for DermotKennedy...\n",
      "pulled 327 for DermotKennedy...\n",
      "pulled 328 for DermotKennedy...\n",
      "pulled 329 for DermotKennedy...\n",
      "pulled 330 for DermotKennedy...\n",
      "pulled 331 for DermotKennedy...\n",
      "pulled 332 for DermotKennedy...\n",
      "pulled 333 for DermotKennedy...\n",
      "pulled 334 for DermotKennedy...\n",
      "pulled 335 for DermotKennedy...\n",
      "pulled 336 for DermotKennedy...\n",
      "pulled 337 for DermotKennedy...\n",
      "pulled 338 for DermotKennedy...\n",
      "pulled 339 for DermotKennedy...\n",
      "pulled 340 for DermotKennedy...\n",
      "finished pulling followers for DermotKennedy.\n",
      "3:22:40.161045\n"
     ]
    }
   ],
   "source": [
    "# Modify the below code stub to pull the follower IDs and write them to a file. \n",
    "handles = ['damianmarley','DermotKennedy']\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "user_data = dict() \n",
    "followers_data = dict()\n",
    "\n",
    "\n",
    "# Grabs the time when we start making requests to the API\n",
    "start_time = datetime.datetime.now()\n",
    "for handle in handles :\n",
    "    pulls = 0\n",
    "    user_data[handle] = []\n",
    "    followers_data[handle] = [] # will be a simple list of IDs\n",
    "    # Create the output file names \n",
    "\n",
    "    followers_output_file = \"twitter/\" + handle + \"_followers.txt\"\n",
    "    users_output_file = \"twitter/\" + handle + \"_followers_data.txt\"\n",
    "    user_obj = client.get_user(username = handle)\n",
    "    print(f'pulling followers for {handle}.')\n",
    "    \n",
    "    \n",
    "        \n",
    "    for followers in tweepy.Paginator(client.get_users_followers, user_obj.data.id, \n",
    "                                      user_fields=['id','username','name','description','location','public_metrics']\n",
    "                                      ,max_results = 1000, limit = 100):\n",
    "        \n",
    "        pulls +=1\n",
    "        print(f'pulled {pulls+1} for {handle}...')\n",
    "        \n",
    "        for user in followers.data:\n",
    "            users = (str(user.username),\n",
    "                     str(user.name),\n",
    "                     str(user.location),\n",
    "                     re.sub(r\"\\s+\",\" \", user.description).strip(),\n",
    "                     str(user.public_metrics['followers_count']),\n",
    "                     str(user.public_metrics['following_count']))\n",
    "            user_data[handle].append(users)\n",
    "        \n",
    "            followersid = (str(user.id))\n",
    "            followers_data[handle].append(followersid)\n",
    "                \n",
    "        #for follower in followers.data:\n",
    "        #fols = (followers.id)\n",
    "        #followers_data[handle].append(fols)\n",
    "            \n",
    "            \n",
    "        # If you've pulled num_followers_to_pull, feel free to break out paged twitter API response\n",
    "        if len(followers_data[handle]) >= num_followers_to_pull:\n",
    "            break\n",
    "            \n",
    "        time.sleep(60)\n",
    "        # Writing the IDs to the output file in the `twitter` folder\n",
    "        with open(users_output_file, 'w', encoding='utf-8') as f1:\n",
    "            headers = '\\t'.join(['username','name','location','description','followers_count','following_count'])\n",
    "            f1.write(headers+\"\\n\")\n",
    "            for user in followers_data:\n",
    "                row = handle + \"\\t\" + \"\\t\".join(str(i) for i in user_data[handle])\n",
    "                f1.write(row+\"\\n\")\n",
    "        with open(followers_output_file, 'w', encoding='utf-8') as f2:\n",
    "            headers = '\\t'.join(['id'])\n",
    "            f2.write(headers+\"\\n\")\n",
    "            for follower in followers_data:\n",
    "                row = handle + \"\\t\" + \"\\t\".join(str(i) for i in followers_data[handle])\n",
    "                f2.write(row+\"\\n\")\n",
    "    print(f'finished pulling followers for {handle}.')\n",
    "       \n",
    "# Let's see how long it took to grab all follower IDs\n",
    "end_time = datetime.datetime.now()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6871eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Home by Warsan Shire no one leaves home unless home is the mouth of a shark. you only run for the border when you see the whole city running as well.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tricky_description = \"\"\"\n",
    "    Home by Warsan Shire\n",
    "    \n",
    "    no one leaves home unless\n",
    "    home is the mouth of a shark.\n",
    "    you only run for the border\n",
    "    when you see the whole city\n",
    "    running as well.\n",
    "\n",
    "\"\"\"\n",
    "# This won't work in a tab-delimited text file.\n",
    "\n",
    "clean_description = re.sub(r\"\\s+\",\" \",tricky_description).strip()\n",
    "clean_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78753e54",
   "metadata": {},
   "source": [
    "## Lyrics Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a62a284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = {'damianmarley':\"https://www.azlyrics.com/d/damianmarley.html\",\n",
    "           'DermotKennedy':\"https://www.azlyrics.com/d/dermotkennedy.html\"} \n",
    "# we'll use this dictionary to hold both the artist name and the link on AZlyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386656a1",
   "metadata": {},
   "source": [
    "## A Note on Rate Limiting\n",
    "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to a Tom Jones song.)\n",
    "\n",
    "Whenever you call requests.get to retrieve a page, put a time.sleep(5 + 10*random.random()) on the next line. This will help you not to get blocked. If you do get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f82c5f",
   "metadata": {},
   "source": [
    "#### Finding links to songs lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d60457e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's set up a dictionary of lists to hold our links\n",
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "for artist, artist_page in artists.items() :\n",
    "    # request the page and sleep\n",
    "    r = requests.get(artist_page)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "\n",
    "    # now extract the links to lyrics pages from this page\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    #for i in soup.find_all('div', {'class': 'listalbum-item'}):\n",
    "        #print(i.find('a')['href'])\n",
    "    links_list = soup.find_all('div', {'class': 'listalbum-item'})\n",
    "    for div in links_list:\n",
    "        lyrics_pages[artist].append(div.find('a')['href'])\n",
    "    #for link in links_list:\n",
    "        #lyrics_pages[artist].append(link.get('href'))\n",
    "        #print(link.get('href'))\n",
    "    # store the links `lyrics_pages` where the key is the artist and the\n",
    "    # value is a list of links. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "65cf97be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('damianmarley', ['/lyrics/damianmarley/trouble.html', '/lyrics/damianmarley/loveandinity.html', '/lyrics/damianmarley/10000chariots.html', '/lyrics/damianmarley/oldwarchant.html', '/lyrics/damianmarley/partytime.html', '/lyrics/damianmarley/kingston12.html', '/lyrics/damianmarley/keepongrooving.html', '/lyrics/damianmarley/searchingsomuchbubble.html', '/lyrics/damianmarley/onemorecupofcoffee.html', '/lyrics/damianmarley/julie.html', '/lyrics/damianmarley/menamejrgong.html', '/lyrics/damianmarley/mrmarley.html', '/lyrics/damianmarley/educatedfools.html', '/lyrics/damianmarley/morejustice.html', '/lyrics/damianmarley/itwaswritten.html', '/lyrics/damianmarley/catchafire.html', '/lyrics/damianmarley/stillsearchin.html', '/lyrics/damianmarley/sheneedsmylove.html', '/lyrics/damianmarley/miblenda.html', '/lyrics/damianmarley/whereisthelove.html', '/lyrics/damianmarley/harderinterlude.html', '/lyrics/damianmarley/borntobewild.html', '/lyrics/damianmarley/givedemsomeway.html', '/lyrics/damianmarley/paradisechild.html', '/lyrics/damianmarley/stuckinbetween.html', '/lyrics/damianmarley/halfwaytree.html', '/lyrics/damianmarley/standachance.html', '/lyrics/damianmarley/confrontation.html', '/lyrics/damianmarley/thereforyou.html', '/lyrics/damianmarley/welcometojamrock.html', '/lyrics/damianmarley/themasterhascomeback.html', '/lyrics/damianmarley/allnight.html', '/lyrics/damianmarley/beautiful.html', '/lyrics/damianmarley/pimpasparadise.html', '/lyrics/damianmarley/move.html', '/lyrics/damianmarley/forthebabies.html', '/lyrics/damianmarley/heygirl.html', '/lyrics/damianmarley/roadtozion.html', '/lyrics/damianmarley/weregonnamakeit.html', '/lyrics/damianmarley/in2deep.html', '/lyrics/damianmarley/khakisuit.html', 'https://www.azlyrics.com/lyrics/nas/asweenter.html', 'https://www.azlyrics.com/lyrics/nas/tribesatwar.html', 'https://www.azlyrics.com/lyrics/nas/strongwillcontinue.html', 'https://www.azlyrics.com/lyrics/nas/leaders.html', 'https://www.azlyrics.com/lyrics/nas/friends.html', 'https://www.azlyrics.com/lyrics/nas/countyourblessings.html', 'https://www.azlyrics.com/lyrics/nas/dispear.html', 'https://www.azlyrics.com/lyrics/nas/landofpromise.html', 'https://www.azlyrics.com/lyrics/nas/inhisownwords.html', 'https://www.azlyrics.com/lyrics/nas/nahmean.html', 'https://www.azlyrics.com/lyrics/nas/patience.html', 'https://www.azlyrics.com/lyrics/nas/mygeneration.html', 'https://www.azlyrics.com/lyrics/nas/africamustwakeup.html', 'https://www.azlyrics.com/lyrics/nas/ancientpeople.html', '/lyrics/damianmarley/intro.html', '/lyrics/damianmarley/herewego.html', '/lyrics/damianmarley/nailponcross.html', '/lyrics/damianmarley/roar.html', '/lyrics/damianmarley/medication.html', '/lyrics/damianmarley/timetravel.html', '/lyrics/damianmarley/livingitup.html', '/lyrics/damianmarley/looksaredeceiving.html', '/lyrics/damianmarley/thestrugglediscontinues.html', '/lyrics/damianmarley/autumnleaves.html', '/lyrics/damianmarley/everybodywantstobesomebody.html', '/lyrics/damianmarley/upholstery.html', '/lyrics/damianmarley/grownsexy.html', 'https://www.azlyrics.com/lyrics/stephenmarley/perfectpicture.html', '/lyrics/damianmarley/soachildmayfollow.html', '/lyrics/damianmarley/slavemill.html', '/lyrics/damianmarley/caution.html', '/lyrics/damianmarley/speaklife.html', '/lyrics/damianmarley/affairsoftheheart.html', '/lyrics/damianmarley/bitterblood.html', '/lyrics/damianmarley/hardwork.html', '/lyrics/damianmarley/holiday.html', '/lyrics/damianmarley/independence.html', '/lyrics/damianmarley/gunmanworld.html', '/lyrics/damianmarley/justaintthesame.html', '/lyrics/damianmarley/lifeisacircle.html', 'https://www.azlyrics.com/lyrics/skrillex/makeitbundem.html', '/lyrics/damianmarley/medicationremix.html', '/lyrics/damianmarley/oneloafofbread.html', '/lyrics/damianmarley/reachhomesafe.html', '/lyrics/damianmarley/redgoldandgreen.html', '/lyrics/damianmarley/setupshop.html', '/lyrics/damianmarley/thedreadful.html', '/lyrics/damianmarley/themission.html']), ('DermotKennedy', ['/lyrics/dermotkennedy/glory.html', '/lyrics/dermotkennedy/allmyfriends.html', '/lyrics/dermotkennedy/acloseness.html', '/lyrics/dermotkennedy/boston.html', '/lyrics/dermotkennedy/youngfree.html', '/lyrics/dermotkennedy/couldnttell.html', '/lyrics/dermotkennedy/aneveningiwillnotforgetfurthestthing.html', '/lyrics/dermotkennedy/swimgood.html', '/lyrics/dermotkennedy/momentspassedmikedeanremix.html', '/lyrics/dermotkennedy/poweroverme.html', '/lyrics/dermotkennedy/momentspassed.html', '/lyrics/dermotkennedy/glory.html', '/lyrics/dermotkennedy/aneveningiwillnotforget.html', '/lyrics/dermotkennedy/forislandfiresandfamily.html', '/lyrics/dermotkennedy/youngfree.html', '/lyrics/dermotkennedy/allmyfriends.html', '/lyrics/dermotkennedy/boston.html', '/lyrics/dermotkennedy/acloseness.html', '/lyrics/dermotkennedy/couldnttell.html', '/lyrics/dermotkennedy/afterrain.html', '/lyrics/dermotkennedy/shelter.html', '/lyrics/dermotkennedy/aneveningiwillnotforget.html', '/lyrics/dermotkennedy/allmyfriends.html', '/lyrics/dermotkennedy/poweroverme.html', '/lyrics/dermotkennedy/whathaveidone.html', '/lyrics/dermotkennedy/momentspassed.html', '/lyrics/dermotkennedy/thecorner.html', '/lyrics/dermotkennedy/lost.html', '/lyrics/dermotkennedy/rome.html', '/lyrics/dermotkennedy/outnumbered.html', '/lyrics/dermotkennedy/dancingunderredskies.html', '/lyrics/dermotkennedy/outgrown.html', '/lyrics/dermotkennedy/redemption.html', '/lyrics/dermotkennedy/withoutfear.html', '/lyrics/dermotkennedy/giants.html', '/lyrics/dermotkennedy/thekillerwasacoward.html', '/lyrics/dermotkennedy/temptationlivefromsoundwaves.html', '/lyrics/dermotkennedy/dayslikethis.html', '/lyrics/dermotkennedy/anylove.html', '/lyrics/dermotkennedy/somethingtosomeone.html', '/lyrics/dermotkennedy/kissme.html', '/lyrics/dermotkennedy/dreamer.html', '/lyrics/dermotkennedy/innocenceandsadness.html', '/lyrics/dermotkennedy/divide.html', '/lyrics/dermotkennedy/homeward.html', '/lyrics/dermotkennedy/onelife.html', '/lyrics/dermotkennedy/betterdays.html', '/lyrics/dermotkennedy/alreadygone.html', '/lyrics/dermotkennedy/blossom.html', '/lyrics/dermotkennedy/betterdaysremix.html', '/lyrics/dermotkennedy/bringmedown.html', 'https://www.azlyrics.com/lyrics/bugzymalone/dontcry.html', '/lyrics/dermotkennedy/drivinghomeforchristmas.html', '/lyrics/dermotkennedy/heartless.html', '/lyrics/dermotkennedy/nothingelsematters.html', 'https://www.azlyrics.com/lyrics/kevingates/power.html', '/lyrics/dermotkennedy/powertriptakecare.html', '/lyrics/dermotkennedy/resolution.html', '/lyrics/dermotkennedy/thiswoodlandwontlisten.html'])])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pages.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5457702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist, lp in lyrics_pages.items():\n",
    "    assert(len(set(lp)) > 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8f3b07f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For damianmarley we have 89.\n",
      "The full pull will take for this artist will take 0.25 hours.\n",
      "For DermotKennedy we have 59.\n",
      "The full pull will take for this artist will take 0.16 hours.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how long it's going to take to pull these lyrics \n",
    "# if we're waiting `5 + 10*random.random()` seconds \n",
    "for artist, links in lyrics_pages.items() : \n",
    "    print(f\"For {artist} we have {len(links)}.\")\n",
    "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b7dd6",
   "metadata": {},
   "source": [
    "#### Pulling Lyrics\n",
    "Now that we have the links to our lyrics pages, let's go scrape them! Here are the steps for this part.\n",
    "\n",
    "    1. Create an empty folder in our repo called \"lyrics\".\n",
    "    2. Iterate over the artists in lyrics_pages.\n",
    "    3. Create a subfolder in lyrics with the artist's name. For instance, if the artist was Cher you'd have lyrics/cher/ in your repo.\n",
    "    4. Iterate over the pages.\n",
    "    5. Request the page and extract the lyrics from the returned HTML file using BeautifulSoup.\n",
    "    6. Use the function below, generate_filename_from_url, to create a filename based on the lyrics page, then write the lyrics to a text file with that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aab7e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename_from_link(link) :\n",
    "    \n",
    "    if not link :\n",
    "        return None\n",
    "    \n",
    "    # drop the http or https and the html\n",
    "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
    "    name = link.replace(\".html\",\"\")\n",
    "\n",
    "    name = name.replace(\"/lyrics/\",\"\")\n",
    "    \n",
    "    # Replace useless chareacters with UNDERSCORE\n",
    "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
    "    \n",
    "    # tack on .txt\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "56213efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Make the lyrics folder here. If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then use shutil.rmtree to remove it and create a new one.\n",
    "dirname = 'lyrics'\n",
    "if not os.path.isdir(dirname): \n",
    "    #shutil.rmtree(\"lyrics/\")\n",
    "     os.mkdir(\"lyrics\")\n",
    "    \n",
    "isdir = os.path.isdir(dirname)\n",
    "print(isdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "39637d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the file is:  damianmarley_trouble.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/trouble.html\n",
      "The name of the file is:  damianmarley_loveandinity.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/loveandinity.html\n",
      "The name of the file is:  damianmarley_10000chariots.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/10000chariots.html\n",
      "The name of the file is:  damianmarley_oldwarchant.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/oldwarchant.html\n",
      "The name of the file is:  damianmarley_partytime.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/partytime.html\n",
      "The name of the file is:  damianmarley_kingston12.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/kingston12.html\n",
      "The name of the file is:  damianmarley_keepongrooving.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/keepongrooving.html\n",
      "The name of the file is:  damianmarley_searchingsomuchbubble.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/searchingsomuchbubble.html\n",
      "The name of the file is:  damianmarley_onemorecupofcoffee.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/onemorecupofcoffee.html\n",
      "The name of the file is:  damianmarley_julie.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/julie.html\n",
      "The name of the file is:  damianmarley_menamejrgong.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/menamejrgong.html\n",
      "The name of the file is:  damianmarley_mrmarley.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/mrmarley.html\n",
      "The name of the file is:  damianmarley_educatedfools.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/educatedfools.html\n",
      "The name of the file is:  damianmarley_morejustice.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/morejustice.html\n",
      "The name of the file is:  damianmarley_itwaswritten.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/itwaswritten.html\n",
      "The name of the file is:  damianmarley_catchafire.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/catchafire.html\n",
      "The name of the file is:  damianmarley_stillsearchin.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/stillsearchin.html\n",
      "The name of the file is:  damianmarley_sheneedsmylove.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/sheneedsmylove.html\n",
      "The name of the file is:  damianmarley_miblenda.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/miblenda.html\n",
      "The name of the file is:  damianmarley_whereisthelove.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/whereisthelove.html\n",
      "The name of the file is:  damianmarley_harderinterlude.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/harderinterlude.html\n",
      "The name of the file is:  damianmarley_borntobewild.txt\n",
      "\thttps://www.azlyrics.com/lyrics/damianmarley/borntobewild.html\n",
      "The name of the file is:  dermotkennedy_glory.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/glory.html\n",
      "The name of the file is:  dermotkennedy_allmyfriends.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/allmyfriends.html\n",
      "The name of the file is:  dermotkennedy_acloseness.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/acloseness.html\n",
      "The name of the file is:  dermotkennedy_boston.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/boston.html\n",
      "The name of the file is:  dermotkennedy_youngfree.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/youngfree.html\n",
      "The name of the file is:  dermotkennedy_couldnttell.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/couldnttell.html\n",
      "The name of the file is:  dermotkennedy_aneveningiwillnotforgetfurthestthing.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/aneveningiwillnotforgetfurthestthing.html\n",
      "The name of the file is:  dermotkennedy_swimgood.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/swimgood.html\n",
      "The name of the file is:  dermotkennedy_momentspassedmikedeanremix.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/momentspassedmikedeanremix.html\n",
      "The name of the file is:  dermotkennedy_poweroverme.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/poweroverme.html\n",
      "The name of the file is:  dermotkennedy_momentspassed.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/momentspassed.html\n",
      "The name of the file is:  dermotkennedy_glory.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/glory.html\n",
      "The name of the file is:  dermotkennedy_aneveningiwillnotforget.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/aneveningiwillnotforget.html\n",
      "The name of the file is:  dermotkennedy_forislandfiresandfamily.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/forislandfiresandfamily.html\n",
      "The name of the file is:  dermotkennedy_youngfree.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/youngfree.html\n",
      "The name of the file is:  dermotkennedy_allmyfriends.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/allmyfriends.html\n",
      "The name of the file is:  dermotkennedy_boston.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/boston.html\n",
      "The name of the file is:  dermotkennedy_acloseness.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/acloseness.html\n",
      "The name of the file is:  dermotkennedy_couldnttell.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/couldnttell.html\n",
      "The name of the file is:  dermotkennedy_afterrain.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/afterrain.html\n",
      "The name of the file is:  dermotkennedy_shelter.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/shelter.html\n",
      "The name of the file is:  dermotkennedy_aneveningiwillnotforget.txt\n",
      "\thttps://www.azlyrics.com/lyrics/dermotkennedy/aneveningiwillnotforget.html\n"
     ]
    }
   ],
   "source": [
    "url_stub = \"https://www.azlyrics.com\" \n",
    "start = time.time()\n",
    "\n",
    "total_pages = 0 \n",
    "\n",
    "for artist in lyrics_pages :\n",
    "\n",
    "    # Use this space to carry out the following steps: \n",
    "    # 1. Build a subfolder for the artist\n",
    "    artist_folder = os.path.join('lyrics', artist)\n",
    "    \n",
    "    if not os.path.isdir(artist_folder):\n",
    "        os.mkdir(artist_folder)\n",
    "    # 2. Iterate over the lyrics pages\n",
    "    for song, link in enumerate(lyrics_pages[artist]):\n",
    "        \n",
    "        song_file = generate_filename_from_link(link)\n",
    "        print('The name of the file is: ', song_file)\n",
    "        leek = os.path.join('lyrics' ,artist, song_file)\n",
    "        \n",
    "    # 3. Request the lyrics page. \n",
    "        lyrics_link = url_stub + link.replace('..','')\n",
    "        print('\\t'+lyrics_link)\n",
    "        req = requests.get(lyrics_link, timeout = 5)\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        req.close()\n",
    "        \n",
    "        # Don't forget to add a line like `time.sleep(5 + 10*random.random())`\n",
    "        # to sleep after making the request\n",
    "        time.sleep(5 + 10*random.random())\n",
    "        \n",
    "    # 4. Extract the title and lyrics from the page.\n",
    "        all_title = soup.findAll(\"title\")\n",
    "        song_title = soup.title.text.split(\"-\")[1].split(\"|\")[0].replace(\"Lyrics\",\"\").strip()\n",
    "        start = soup.text.find(f'\"{song_title}\"')\n",
    "        end = soup.text.find('Submit Corrections')\n",
    "        \n",
    "    # 5. Write out the title, two returns ('\\n'), and the lyrics. Use `generate_filename_from_url`\n",
    "    #    to generate the filename. \n",
    "        #song_file = generate_filename_from_link(link)\n",
    "        #print('The name of the file is: ', song_file)\n",
    "        #leek = os.path.join('lyrics' ,artist, song_file)\n",
    "        with open(leek, 'w') as file:\n",
    "            file.write(song_title + '\\n\\n')\n",
    "            for line in soup.text[start:end].split('\\n')[11:]:\n",
    "                file.write(line + '\\n')\n",
    "        file.close()\n",
    "    # Remember to pull at least 20 songs per artist. It may be fun to pull all the songs for the artist\n",
    "        if song >= 20:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "94d3f137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 461982.02 hours.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3ac7fb",
   "metadata": {},
   "source": [
    "### Checking Twitter Data\n",
    "The output from your Twitter API pull should be two files per artist, stored in files with formats like cher_followers.txt (a list of all follower IDs you pulled) and cher_followers_data.txt. These files should be in a folder named twitter within the repository directory. This code summarizes the information at a high level to help the instructor evaluate your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "85c296f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see two artist handles: damianmarley and DermotKennedy.\n"
     ]
    }
   ],
   "source": [
    "twitter_files = os.listdir(\"twitter\")\n",
    "twitter_files = [f for f in twitter_files if f != \".DS_Store\"]\n",
    "artist_handles = list(set([name.split(\"_\")[0] for name in twitter_files]))\n",
    "\n",
    "print(f\"We see two artist handles: {artist_handles[0]} and {artist_handles[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e346d880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see 1 in your follower file for damianmarley, assuming a header row.\n",
      "In the follower data file (damianmarley_followers_data.txt) for damianmarley, we have these columns:\n",
      "username : name : location : description : followers_count : following_count\n",
      "\n",
      "We have 1 data rows for damianmarley in the follower data file.\n",
      "For damianmarley we have 1 unique locations.\n",
      "For damianmarley we have 0 words in the descriptions.\n",
      "Here are the five most common words:\n",
      "[]\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "We see 2 in your follower file for DermotKennedy, assuming a header row.\n",
      "In the follower data file (DermotKennedy_followers_data.txt) for DermotKennedy, we have these columns:\n",
      "username : name : location : description : followers_count : following_count\n",
      "\n",
      "We have 2 data rows for DermotKennedy in the follower data file.\n",
      "For DermotKennedy we have 1 unique locations.\n",
      "For DermotKennedy we have 0 words in the descriptions.\n",
      "Here are the five most common words:\n",
      "[]\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for artist in artist_handles :\n",
    "    follower_file = artist + \"_followers.txt\"\n",
    "    follower_data_file = artist + \"_followers_data.txt\"\n",
    "    \n",
    "    ids = open(\"twitter/\" + follower_file,'r').readlines()\n",
    "    \n",
    "    print(f\"We see {len(ids)-1} in your follower file for {artist}, assuming a header row.\")\n",
    "    \n",
    "    with open(\"twitter/\" + follower_data_file,'r', encoding='utf-8') as infile :\n",
    "        \n",
    "        # check the headers\n",
    "        headers = infile.readline().split(\"\\t\")\n",
    "        \n",
    "        print(f\"In the follower data file ({follower_data_file}) for {artist}, we have these columns:\")\n",
    "        print(\" : \".join(headers))\n",
    "        \n",
    "        description_words = []\n",
    "        locations = set()\n",
    "        \n",
    "        \n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            \n",
    "            try : \n",
    "                locations.add(line[3])            \n",
    "                description_words.extend(words(line[6]))\n",
    "            except :\n",
    "                pass\n",
    "    \n",
    "        \n",
    "\n",
    "        print(f\"We have {idx+1} data rows for {artist} in the follower data file.\")\n",
    "\n",
    "        print(f\"For {artist} we have {len(locations)} unique locations.\")\n",
    "\n",
    "        print(f\"For {artist} we have {len(description_words)} words in the descriptions.\")\n",
    "        print(\"Here are the five most common words:\")\n",
    "        print(Counter(description_words).most_common(5))\n",
    "\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"-\"*40)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0e751",
   "metadata": {},
   "source": [
    "## For the section above, something is not correct because in the text file I have way more than 1 followers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649abf6",
   "metadata": {},
   "source": [
    "### Checking Lyrics\n",
    "The output from your lyrics scrape should be stored in files located in this path from the directory: /lyrics/[Artist Name]/[filename from URL]. This code summarizes the information at a high level to help the instructor evaluate your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "38bbffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For damianmarley we have 41 files.\n",
      "For damianmarley we have roughly 106712 words, 80 are unique.\n",
      "For DermotKennedy we have 15 files.\n",
      "For DermotKennedy we have roughly 27044 words, 68 are unique.\n"
     ]
    }
   ],
   "source": [
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
    "            artist_words.extend((infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7248dba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819ef21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
